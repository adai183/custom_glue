{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Glue\n",
    "> glue between fastai and hugging face's transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from fastai.text.transform import BaseTokenizer, Vocab\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Collection\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CustomTransformerModel(torch.nn.Module):\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Return only the logits from the transfomer\n",
    "        logits = self.transformer(input_ids)[0]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pretrained_tokenizer: PreTrainedTokenizer, model_type=\"bert\", **kwargs\n",
    "    ):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = pretrained_tokenizer.max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t: str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        if self.model_type in [\"roberta\"]:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[\n",
    "                : self.max_seq_len - 2\n",
    "            ]\n",
    "        else:\n",
    "            tokens = self._pretrained_tokenizer.tokenize(t)[: self.max_seq_len - 2]\n",
    "        return [CLS] + tokens + [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos=[])\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def numericalize(self, t: Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        # return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums: Collection[int], sep=\" \") -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return (\n",
    "            sep.join(self.tokenizer.convert_ids_to_tokens(nums))\n",
    "            if sep is not None\n",
    "            else self.tokenizer.convert_ids_to_tokens(nums)\n",
    "        )\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\"itos\": self.itos, \"tokenizer\": self.tokenizer}\n",
    "\n",
    "    def __setstate__(self, state: dict):\n",
    "        self.itos = state[\"itos\"]\n",
    "        self.tokenizer = state[\"tokenizer\"]\n",
    "        self.stoi = collections.defaultdict(\n",
    "            int, {v: k for k, v in enumerate(self.itos)}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (nbdev)",
   "language": "python",
   "name": "nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
